{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZ6ssl1-zD8s"
   },
   "source": [
    "\n",
    "In the Google shared drive (/602/data), the file enron.txt is a subset of the Enron Corpus, a collection of over 500,000 emails from senior management of Enron Corporation leading to its collapse in 2001.  The subset comprises the text of about 15,000 emails available through the TensorFlow Data Set (TFDS) source aeslc (annotated Enron Subject Line Corpus).\n",
    "\n",
    "Using this dataset, construct a neural net that will generate 50 random characters, beginning with the sequence \\verb!The!, that are generated from the distribution of text in the file.\n",
    "\n",
    "This exercise can be replicated using any of the following sources in the texts and documentation:\n",
    "\n",
    "* **Raschka** - Character-level language modeling in TensorFlow, pages 600-613\n",
    "* **GÃ¨ron** - Generating Shakespearean Text Using a Character RNN, pages 526-534\n",
    "* **TensorFlow documentation** [Text Generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation)\n",
    "\n",
    "\n",
    "Adjust the temperature ($\\alpha$ in Raschka) to avoid repeating text.  Using a GPU runtime to fit the model is advised, which may still require several hours to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9YSFsVRzCY9",
    "outputId": "4885bc5d-0138-4257-cf11-8a0c212e9c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import numpy as np\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tStYGXanzEWb"
   },
   "outputs": [],
   "source": [
    "#read the file content into the variable corpus\n",
    "with open('/content/drive/Shareddrives/DS602-S22/Data/enron.txt', 'r', encoding='utf8') as f:\n",
    "  corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uz_BDAd9rznP",
    "outputId": "3d7e0d42-0d27-4f1e-f5e5-62248e47e2dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greg and Mark:  Attached is a draft of the very short story that will accompany your profiles in Enron Business.\n",
      "(PR management has approved.)\n",
      "The purpose is simply to introduce you and quickly address the issue that's on everyone's mind, the stock price.\n",
      "Please review your individual quotes and let me know if you have any revisions as soon as possible.\n",
      "The quotes were extracted from your interviews with us a few weeks ago.\n",
      "If I have not heard back from you by the end of the day tomorrow, I'll assume it's OK to print as is.\n",
      "Thanks again for taking the time to meet with us.\n",
      "Warm regards and cheers,\n",
      "Please note the change in location for tomorrow's meeting.\n",
      "EB4102 is undergoing construction for several months.\n",
      "Ops Committee Meeting \t\tFriday, 10/5 \t\t1:30 - 2:30 PM \t\tEB30C2 - OMA 3C1  Thanks.\n",
      "John/John/Frank  As we are approaching the November 1st Trading Track interview dates, we need to finalize the reminder of the external candidate's initial screen.\n",
      "Can you please provide me with your \n"
     ]
    }
   ],
   "source": [
    "print(corpus[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Khw1N2aEnndg"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TlvT6eD_rlNm"
   },
   "outputs": [],
   "source": [
    "enron_data='/content/drive/Shareddrives/DS602-S22/Data/enron.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "faUO1sWhqEDl",
    "outputId": "9a36fd45-2929-40a0-f117-bb7279e08229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 14442507 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(enron_data, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-Ft8O6Ktnp0",
    "outputId": "3ac6db73-028e-4bec-fbe2-2ce718063bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sOp9sYiivEAp"
   },
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "90FkyXnqvaWd"
   },
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkPWdMCJvsxJ",
    "outputId": "7d4eae09-ffa0-4acb-d7a8-76133238b9f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(14442507,), dtype=int64, numpy=array([43, 85, 72, ..., 86,  3,  2])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ids=ids_from_chars(tf.strings.unicode_split(text, input_encoding='UTF-8'))\n",
    "text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHYMrXEExbno",
    "outputId": "80299f65-3de0-4298-b6d5-5e21ec06d0dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tf.data.Dataset.from_tensor_slices(text_ids)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOkjzyR3x3Pz",
    "outputId": "b8aef2de-b757-4145-e84f-cc51a7f5b7fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G r e g   a n d   M a r k :   "
     ]
    }
   ],
   "source": [
    "for i in input.take(15):\n",
    "    print(chars_from_ids(i).numpy().decode('utf-8'),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bVO9KFb34kLM",
    "outputId": "6428d863-47e6-497d-922d-1c93dd7ec5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178302\n"
     ]
    }
   ],
   "source": [
    "seq_length = 80\n",
    "ex_per_epoch = len(text)//(seq_length+1)\n",
    "print(ex_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yzk0Gkns59Mk",
    "outputId": "00e8cb8a-3936-4628-9803-c916da714d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'G' b'r' b'e' b'g' b' ' b'a' b'n' b'd' b' ' b'M' b'a' b'r' b'k' b':'\n",
      " b' ' b' ' b'A' b't' b't' b'a' b'c' b'h' b'e' b'd' b' ' b'i' b's' b' '\n",
      " b'a' b' ' b'd' b'r' b'a' b'f' b't' b' ' b'o' b'f' b' ' b't' b'h' b'e'\n",
      " b' ' b'v' b'e' b'r' b'y' b' ' b's' b'h' b'o' b'r' b't' b' ' b's' b't'\n",
      " b'o' b'r' b'y' b' ' b't' b'h' b'a' b't' b' ' b'w' b'i' b'l' b'l' b' '\n",
      " b'a' b'c' b'c' b'o' b'm' b'p' b'a' b'n' b'y' b' ' b'y'], shape=(81,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = input.batch(seq_length+1, drop_remainder=True)\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7mnKSGeD7NDM"
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gb0-zY5X7yo4",
    "outputId": "730f442b-41d9-46ad-ca90-cae5dfcef81f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Greg and Mark:  Attached is a draft of the very short story that will accompany y'\n",
      "b'our profiles in Enron Business.\\r\\n(PR management has approved.)\\r\\nThe purpose is si'\n",
      "b\"mply to introduce you and quickly address the issue that's on everyone's mind, th\"\n",
      "b'e stock price.\\r\\nPlease review your individual quotes and let me know if you have '\n",
      "b'any revisions as soon as possible.\\r\\nThe quotes were extracted from your interview'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "561DDcbH8xeP"
   },
   "outputs": [],
   "source": [
    "def target_split(seq):\n",
    "  return seq[:-1], seq[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "b8bY3gsNOAl2"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(target_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJ_vxAmX9Kuh",
    "outputId": "7a4d8916-62e5-4333-b94e-02eed58508fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'Greg and Mark:  Attached is a draft of the very short story that will accompany '\n",
      "Target: b'reg and Mark:  Attached is a draft of the very short story that will accompany y'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1XTGi5gQugv",
    "outputId": "e9ecd884-3bb3-48f1-c6ec-0d3457029f76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 80), dtype=tf.int64, name=None), TensorSpec(shape=(64, 80), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9ssaTCw2Qyyc"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NLxw9wmNQ6d4"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dsYKP-SKQ-qK"
   },
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQqMFx_mRBPd",
    "outputId": "f2a8716a-333c-45ee-fc4c-b30e941cff58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 80, 98) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbIpN-9aRFG4",
    "outputId": "0f4966a1-e0fd-4f73-ec1c-8d1b33de3c0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  25088     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  100450    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,063,842\n",
      "Trainable params: 4,063,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbp-LRH0RH_I",
    "outputId": "4e97bd69-bcdf-4d51-c00c-6bab3d076f16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32, 83, 91, 25, 75, 12, 42, 73, 22, 94, 64, 47, 33, 51, 55, 81, 32,\n",
       "       84, 67, 96, 27, 38, 21, 36, 32, 35, 76, 43, 31, 53, 82, 84, 22, 94,\n",
       "       12, 40, 85, 37, 33,  5, 66, 95, 74, 37, 39, 33, 41, 90, 11, 53, 24,\n",
       "        9, 54, 84, 78, 14, 71, 25, 96, 12, 35, 89, 65, 27,  1, 97, 21, 23,\n",
       "       66, 25,  2, 19, 59, 13, 21, 51, 34, 65, 15, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDIMjN01RNBX",
    "outputId": "988a9f71-8643-44ee-9e62-52c52505d887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'urate documents are distributed, please allow us to prepare the drafts for distr'\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"<px5h(Ff2{\\\\K=OSn<q`}7B1@<?iG;Qoq2{(DrA=!_|gAC=Ew'Q4%Rqk*d5}(?v]7\\t~13_5\\n/W)1O>]+,\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HqnBdcasUN-X"
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Fx_y4DnDRUSM"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "uH6dYO8zRt4W"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiAcVfStSpJp",
    "outputId": "f7df3493-69e0-4025-9e12-e6865f6066b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2785/2785 [==============================] - 316s 111ms/step - loss: 1.4372\n",
      "Epoch 2/10\n",
      "2785/2785 [==============================] - 307s 110ms/step - loss: 1.1018\n",
      "Epoch 3/10\n",
      "2785/2785 [==============================] - 309s 110ms/step - loss: 1.0447\n",
      "Epoch 4/10\n",
      "2785/2785 [==============================] - 308s 110ms/step - loss: 1.0153\n",
      "Epoch 5/10\n",
      "2785/2785 [==============================] - 311s 111ms/step - loss: 0.9990\n",
      "Epoch 6/10\n",
      "2785/2785 [==============================] - 310s 111ms/step - loss: 0.9898\n",
      "Epoch 7/10\n",
      "2785/2785 [==============================] - 310s 111ms/step - loss: 0.9853\n",
      "Epoch 8/10\n",
      "2785/2785 [==============================] - 311s 111ms/step - loss: 0.9841\n",
      "Epoch 9/10\n",
      "2785/2785 [==============================] - 310s 111ms/step - loss: 0.9857\n",
      "Epoch 10/10\n",
      "2785/2785 [==============================] - 310s 111ms/step - loss: 0.9897\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "LTbVAElrSp7b"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Aocj6Ug-iPR8"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQaGMhSRjUOE",
    "outputId": "e5ff4721-af71-4843-8f1d-8f6c6b44d4ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fa76ae38d50>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fa76ae38d50>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0agU61Wgyrpi",
    "outputId": "b07e4235-f3a0-48b9-a532-5adea4a84c76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'The explosion \"allow yet along then bagance of doesn\\'t\".\\r\\nNext meeating, Enron would have to specify our best in the world, but I think the caller we want to build cheat from Sengency in sutcome certain that this product is still for nothing, How added that there is now a things next Fred and she would give you my questions.\\r\\nWe have last specific on the lunch with  more late time for this.\\r\\nLet me prip a chance to look atmeal --3 and Marc,  team.\\r\\nI think it\\'s now.\\r\\nTo do a Termination EreatR  9/18/00)     TETCO WARS TO GET THROUGY AT BC ACTELS.ONT TA TALE ME TO ENPLAYERES METER IN THE ENTERSATION INTURATED WIAL FRODBW. 11:00 AM CHECIFIMED THAN AOLARD SHATE) will occas TW matter agreements to the IRSC estate.\\r\\nThe subject of this master netwing the  obligation contained in the current funuse to  you up.\\r\\nAs you should now hear, but I want  to find this to let you know what projects from Enron While are you again and what books from hoist of this year at Enron.\\r\\nI Could be out of up to Lin'\n",
      " b\"The long time divisions as to what it would be content for the move.\\r\\nPPS submitted out the higher  fees from the utilities for Enron's music and other counterparties have been cancelled the state of EOL capacity learned by asher or requiring the consulting.\\r\\nWe are show the price capacity for each change in favor and compare up al being reviewed by Williams Bound, or bill the remiance during phone, but I want to change it, at his birthday will not go add it.\\r\\nb.\\r\\n.\\r\\nGeneral Counterparty, will all burn carefully.\\r\\nInvolinely, Lisa Petrome\\r\\nJeff.\\r\\nThanks.\\r\\nThey are open, rules the 20th floor confrict.\\r\\nLet me know if so that you receive from Kater Ricked\\r\\nI am  talking about on the deal at home Points crazy California confirmed in Detegrial Hedge Ray  of  the system after our swap (strip at  a laptop  plant).\\r\\nWe do not keep the PM sign-off all employees as soon as possible.\\r\\nFrank Joempato at 3-7000.\\r\\nMark:  Please check on the phase of your request that I call you go for a product with th\"\n",
      " b\"The Alumni / the parties that I should have about = of our regard November of the paperworking against the offer.\\r\\nAs of team, who would be file of collateral to several months with respect to any hardware and click the Accelents and Valentate Logistics team.\\r\\nA meeting will be a place meeting with such a promotal basic to hit Enron's legal teals  even in support of cooperation with a four.\\r\\nThey can provide confirming what has the relevant draft Of Potter ?\\r\\nGlobal creditors are unable to review the definitions of a Personal Carow Environme.\\r\\nMike\\r\\nAttached are the IRSs: West Texal to NETCO Example: No Mahoundaded databases, TV is the file,\\r\\nFor-paining the fact that the Orisinstor These Canadian executed billing provides due to California Power w/our current situation after ENA.\\r\\nDon't go also access to celebrating opportunities.\\r\\nThis is the imminet that ever changed with trading daily, I  would clear it or non).\\r\\n8.\\r\\nRegarding the trades which are play the form of finish source of equi\"\n",
      " b\"The change had been more daily to establish an applicable liter.\\r\\nThat structure to schedule the change ontormation listed below:  As we right cliff, anyward this job.\\r\\nNote to talk to Lou.\\r\\nJesus\\r\\nI should be accurate bonuses from raising the duplicate  typ WIst Central ballet in the fact type of weather only) around I need to be working on  us?\\r\\nI hope that you dad this time.\\r\\nI call you for info.\\r\\nIf you have troster to go, speaking until November 5, 7, 2001, the Boil EVP1  and I might now be bringly thought that your biofe will be home and your folder while the ISDA may now require assignment a= nd structuring a steps to pay the title sub= ed=20 for the one in.\\r\\n(See gibth from our LOG), and other shape wooked in detail of manufacts from the ARMP's authority.\\r\\nI had done that support will sign the loans from Intion to work  ihad as  not to be reface available to him (what we,      )  don\\r\\nThank you for your sourcing going from the Four Requirements.\\r\\nThis is the assimum to how this cou\"\n",
      " b\"The revenue  is FTM-season and other exposure (just killington, demand\\\\ wine's title group toral) if FERC has already fit  all the counterparts (on a position political authorical idea) for PA's to the state of one number sheet.\\r\\n=20 Heart based on the permit, the Federal Expansion, you do not discontinue to use a vital as parking or there is no nature of any contact  deals.\\r\\nHarvardient to  him the understood this  while you well in a possible pissing, these pipts in a presentation from wedphical policy of goods.\\r\\nPlease talk nit your number about as the Gulf Cre today for the courcing coggoname for the meeting.\\r\\nYou can now have  old location of data she  is.\\r\\nIn addition, you may like a meeting, Ranky and Rickyay and La Cash and I spoke with the Check Caub we have also been here  Hadley offer you over, this is to file the Enron party as keying of a party with the process of availability.\\r\\nThis outage is to formoregy agreements in the program and PACHoutles?\\r\\ndo like the gets check with \"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 6.50773549079895\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['The ', 'The ', 'The ', 'The ', 'The '])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWzc__yUzdiI",
    "outputId": "d7a70392-9407-40c1-ab60-8058bb89768c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  language should print put their last spreads between Credit Watch Hills Bricing Dierest Name a.\r\n",
      "- \n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['The '])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_whcKLNGKlS5"
   },
   "source": [
    "## Reference:\n",
    "\n",
    "### 1. https://www.tensorflow.org/text/tutorials/text_generation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Swapan_Gupta_Data_602_Week12.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
